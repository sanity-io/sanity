import {tokenize} from '../src/search/common/tokenize'

const tests = [
  ['', []],
  ['foo', ['foo']],
  ['0foo', ['0foo']],
  ['a16z', ['a16z']],
  ['foo,,, ,    ,foo,bar', ['foo', 'foo', 'bar']],
  ['pho-bar, foo-bar', ['pho', 'bar', 'foo', 'bar']],
  ['0 foo', ['0', 'foo']],
  ['foo ðŸ¤ªðŸ¤ªðŸ¤ª', ['foo', 'ðŸ¤ªðŸ¤ªðŸ¤ª']],
  ['foo ðŸ¤ªðŸ¤ªðŸ¤ª bar', ['foo', 'ðŸ¤ªðŸ¤ªðŸ¤ª', 'bar']],
  ['1 2 3', ['1', '2', '3']],
  ['foo, bar, baz', ['foo', 'bar', 'baz']],
  ['foo   , bar   , baz', ['foo', 'bar', 'baz']],
  ['a.b.c', ['a.b.c']],
  ['sanity.io', ['sanity.io']],
  ['fourty-two', ['fourty', 'two']],
  ['full stop. Then new beginning', ['full', 'stop', 'Then', 'new', 'beginning']],
  ['about .io domains', ['about', 'io', 'domains']],
  ['abc -23 def', ['abc', '23', 'def']],
  ['banana&[friends]\\/ barnÃ¥ler', ['banana', 'friends', 'barnÃ¥ler']],
  ['banana&friends barnÃ¥ler', ['banana', 'friends', 'barnÃ¥ler']],
  ['ban*ana*', ['ban', 'ana']],
  ['í•œêµ­ì¸ì€ banana ë™ì˜í•˜ì§€ ì•ŠëŠ”ë‹¤', ['í•œêµ­ì¸ì€', 'banana', 'ë™ì˜í•˜ì§€', 'ì•ŠëŠ”ë‹¤']],
  ['í•œêµ­ì¸ì€    ë™ì˜2í•˜ì§€', ['í•œêµ­ì¸ì€', 'ë™ì˜2í•˜ì§€']],
]

tests.forEach(([input, expected]) => {
  test('tokenization of search input string', () => {
    expect(tokenize(input)).toEqual(expected)
  })
})
