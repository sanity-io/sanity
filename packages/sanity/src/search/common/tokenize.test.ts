import {tokenize} from './tokenize'

const tests = [
  {input: '', expected: []},
  {input: 'foo', expected: ['foo']},
  {input: '0foo', expected: ['0foo']},
  {input: 'a16z', expected: ['a16z']},
  {input: 'foo,,, ,    ,foo,bar', expected: ['foo', 'foo', 'bar']},
  {input: 'pho-bar, foo-bar', expected: ['pho', 'bar', 'foo', 'bar']},
  {input: '0 foo', expected: ['0', 'foo']},
  {input: 'foo ðŸ¤ªðŸ¤ªðŸ¤ª', expected: ['foo', 'ðŸ¤ªðŸ¤ªðŸ¤ª']},
  {input: 'foo ðŸ¤ªðŸ¤ªðŸ¤ª bar', expected: ['foo', 'ðŸ¤ªðŸ¤ªðŸ¤ª', 'bar']},
  {input: '1 2 3', expected: ['1', '2', '3']},
  {input: 'foo, bar, baz', expected: ['foo', 'bar', 'baz']},
  {input: 'foo   , bar   , baz', expected: ['foo', 'bar', 'baz']},
  {input: 'a.b.c', expected: ['a.b.c']},
  {input: 'sanity.io', expected: ['sanity.io']},
  {input: 'fourty-two', expected: ['fourty', 'two']},
  {input: 'full stop. Then new beginning', expected: ['full', 'stop', 'Then', 'new', 'beginning']},
  {input: 'about .io domains', expected: ['about', 'io', 'domains']},
  {input: 'abc -23 def', expected: ['abc', '23', 'def']},
  {input: 'banana&[friends]\\/ barnÃ¥ler', expected: ['banana', 'friends', 'barnÃ¥ler']},
  {input: 'banana&friends barnÃ¥ler', expected: ['banana', 'friends', 'barnÃ¥ler']},
  {input: 'ban*ana*', expected: ['ban', 'ana']},
  {
    input: 'í•œêµ­ì¸ì€ banana ë™ì˜í•˜ì§€ ì•ŠëŠ”ë‹¤',
    expected: ['í•œêµ­ì¸ì€', 'banana', 'ë™ì˜í•˜ì§€', 'ì•ŠëŠ”ë‹¤'],
  },
  {input: 'í•œêµ­ì¸ì€    ë™ì˜2í•˜ì§€', expected: ['í•œêµ­ì¸ì€', 'ë™ì˜2í•˜ì§€']},
]

tests.forEach(({input, expected}) => {
  test('tokenization of search input string', () => {
    expect(tokenize(input)).toEqual(expected)
  })
})
